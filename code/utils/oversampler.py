import math
import torch
from torch.utils.data import Dataset, DataLoader, Sampler
from imblearn.over_sampling import SMOTE
import numpy as np
import random

# Step 1: Define the dataset
class CustomDataset(Dataset):
    def __init__(self, data, labels, features):
        self.data = data
        self.labels = labels
        self.features = features

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx], self.features[idx]

    def get_features_and_labels(self):
        """
        Returns the features and labels for SMOTE processing.
        """
        return self.data.numpy(), self.labels.numpy()

    def extend_with_synthetic(self, synthetic_data, synthetic_labels, synthetic_features):
        """
        Extend the dataset with synthetic samples generated by SMOTE.
        """
        self.data = torch.cat((self.data, torch.tensor(synthetic_data, dtype=torch.float)))
        self.labels = torch.cat((self.labels, torch.tensor(synthetic_labels, dtype=torch.long)))
        self.features = torch.cat((self.features, torch.tensor(synthetic_features, dtype=torch.long)))

# Step 2: Create the dataset
data = torch.randn(1000, 10)  # 1000 samples, 10 features each
labels = torch.randint(0, 3, (1000,))  # Class labels
feature_of_interest = torch.randint(0, 5, (1000,))  # Feature with 5 unique values
dataset = CustomDataset(data, labels, feature_of_interest)

# Step 3: Apply SMOTE to the dataset
X, y = dataset.get_features_and_labels()
feature_to_smote = dataset.features.numpy().reshape(-1, 1)  # Combine the feature with the data for SMOTE
smote = SMOTE(k_neighbors=5, random_state=42)

# Combine features and data as input to SMOTE
X_smote, y_smote = smote.fit_resample(np.hstack((X, feature_to_smote)), y)

# Separate the synthetic features from the main data
synthetic_data = X_smote[:, :-1]  # Exclude the feature column
synthetic_features = X_smote[:, -1]  # Only the feature column
synthetic_labels = y_smote

# Extend the dataset with synthetic data
dataset.extend_with_synthetic(synthetic_data, synthetic_labels, synthetic_features)

# Step 4: Create a custom batch sampler
class FeatureBasedBatchSampler(Sampler):
    def __init__(self, dataset, feature_extractor, batch_size):
        super(FeatureBasedBatchSampler, self).__init__()
        self.dataset = dataset
        self.feature_extractor = feature_extractor
        self.batch_size = batch_size
        self.feature_to_indices = self._group_by_feature()
        self.num_samples = len(dataset)

    def _group_by_feature(self):
        feature_to_indices = {}
        for idx in range(len(self.dataset)):
            _, _, feature = self.dataset[idx]
            if feature not in feature_to_indices:
                feature_to_indices[feature] = []
            feature_to_indices[feature].append(idx)
        return feature_to_indices

    def __iter__(self):
        indices = []
        for feature_indices in self.feature_to_indices.values():
            repeat_count = math.ceil(self.num_samples / len(feature_indices))
            oversampled_indices = feature_indices * repeat_count
            indices.extend(oversampled_indices[:self.num_samples])

        random.shuffle(indices)
        for i in range(0, len(indices), self.batch_size):
            yield indices[i:i + self.batch_size]

    def __len__(self):
        return len(self.dataset) // self.batch_size

# Step 5: Use the custom sampler in a DataLoader
batch_sampler = FeatureBasedBatchSampler(dataset, feature_extractor=lambda x: x[2], batch_size=32)
dataloader = DataLoader(dataset, batch_sampler=batch_sampler)

# Training loop example
for batch_data, batch_labels, batch_features in dataloader:
    print("Batch data shape:", batch_data.shape)
    print("Batch feature distribution:", torch.unique(batch_features, return_counts=True))
    break
